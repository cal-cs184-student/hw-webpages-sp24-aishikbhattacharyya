<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184/284A Pathtracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184/284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Homework 3: Pathtracer</h1>
<h2 align="middle">Aishik Bhattacharyya</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
	
In this project, I created my own rasterizer with various sampling methods and analyzed its efficacy in the context of every method. I was able to understand why some techniques are better from different perspectives like memory and speed. It was also really cool to zoom into the images and play around with different methods and see, in real time, the stark differences.

<h2 align="middle">Part I: Ray Generation and Scene Generation</h2>

In the rendering pipeline, ray generation and primitive intersection both play very important roles. For ray generation, the end goal is to transform a two-dimensional coordinate in image space to a three-dimensional coordinate in camera space. The z-coordinate in camera space is set to -1 to maintain a consistent depth as this direction represents the viewing direction. To transform the x-coordinate, we take the horizontal field of view value in radians, divide it by 2, and take the negative tangent. As seen in the provided Camera::configure() function, we also need to multiply this by 2. The rationale behind this is that our field of view values are represented as half angles. We follow the same steps for the vertical field of view to get a three-dimensional vector. To convert, we apply the camera-to-world rotation matrix and normalize the resulting value for consistency to finally get our ray direction. To sum up the resulting ray, we have found the normalized ray direction and also note the given camera position. We also set lower and upper bounds for the ray to maintain bounds and set intersection values later. Now, with ray generation complete, we must apply this method in a practical setting. We can use this to estimate the integral of radiance over a pixel using Monte Carlo. Using the provided function Pathtracer::get_sample(), we get a two-dimensional vector to offset from our origin. We normalize these coordinates by the width and height of the image and pass it into the previous function we created. Then, we take this result, get the estimated radiance and add it to a running total. Finally, when we’ve completed sampling, we divide the running total by the number of samples and it to our sample buffer.

Calculating primitive intersection parts of the rendering pipeline is also important because it determines what the camera captures, including important features such as shadows and visibility. In this section, we implement two methods regarding intersections with triangles and spheres. One of them simply checks whether or not an intersection exists, the other does that and also returns information regarding any such intersection. The triangle intersection algorithm implements the Moller Trumbore Algorithm. This algorithm relies on finding the barycentric coordinates to determine where the ray intersects the triangle. It also returns a value, t, which determines the magnitude in the ray where the intersection occurs.  We check to see if the value t is greater than or equal to 0 and within bounds of the min and max values of the ray. We also check if the barycentric coordinates are between 0 and 1, inclusive. If these are all true then, there is an intersection and we can calculate the normal there by multiplying the barycentric coordinates and the mesh normals of the triangle.

<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="CBempty.png" align="middle" width="400px"/>
        <figcaption align="middle">CBEmpty</figcaption>
      </td>
      <td>
        <img src="CBspheres.png" align="middle" width="400px"/>
        <figcaption align="middle">CBSpheres</figcaption>
      </td>
    </tr>
  </table>
</div>


<h2 align="middle">Part II: Bounding Volume Hiearchy</h2>
	
The purpose of the BVH construction algorithm is to reorganize the geometry in question through bounding boxes and reduce the number of intersections to check for. First, given a list of primitives, we construct a bounding box and initialize a node containing this final result. Depending on how many primitives we traversed, we compare it to the inputted maximum leaf size to determine whether or not the node we created is a leaf node. If so, we modify the node’s start and end values to what we just looped over. The importance of this step is to check if the number of primitives is below a certain threshold where we can just perform intersection checks regularly. Then, if our number of primitives exceeds this value, we have an algorithm that will split spaces within the image to optimize the number of intersection checks required. This is super important for large, complex geometries. First, we want to find which axis to split on, or the one that gives us the most benefit. To do this I retrieved the extent of the bounding box previously calculated. From the extent vector, I selected the axis which had the greatest value. For the splitting point, I calculated the average of all the centroids within the bounding box I created and defined two new primitive iterators, which represent the left and right subsection I will recursively iterate. To determine which value gets put inside which iterator, I check whether each value of the centroid’s axis to split on is less than the average centroid’s axis to split. If so, it goes in the left vector, or else it goes into the right vector. I chose the mean because I could calculate it in the same loop as calculating the bounding box, which saves a lot of time. I valued time a lot in this case because the larger the geometries get, the more amplified inefficient algorithms show. Furthermore, the spec warned that there are some cases in which all primitives might lie on one side of the split point, there might be a segfault. In most cases, choosing the mean as a heuristic is safe, but not in all cases. To compensate for this, I check if any of the primitive vectors are empty, and, if so, based on it being left or right, I pop the front or back of the vector that’s not empty and push it onto the one that is empty. As a result, before we begin the recursive step, the size of each vector is at least one and will not lead to a segfault. Then, I create two new iterators that help assign the order of the left and right elements that’ll be used in the recursive step. When calling this method on the left side, we loop from the input start to our new iterator and, for the right side, we loop from the new iterator to the input end.

Testing image rendering on images with and without BVH acceleration resulted in staggering results. First, I tested the maxplanck.dae image inside the meshedit folder. Without BVH acceleration, it took my computer a long time to run and produced an average speed of 0.0011 million rays per second. With BVH acceleration, it produced an average speed of 0.1158 million rays per second. The difference between this is 100x. This is a fairly complex geometry of a face-like sculpture. In a more advanced test case of a sculpture, I tested the CBlucy.dae image inside the sky folder. Here, without BVH acceleration, it took 0.0004 million rays per second, and with BVH acceleration, it took 0.2158 million rays per second. The difference between these is more than 700x. We can see that as geometries get more complex, the greater the time impact BVH accelerations have on rendering the image.
  <div align="middle">
  <table style="width=100%">
	<tr align="center">
      <td>
        <img src="max.png" align="middle" width="400px"/>
        <figcaption align="middle">maxplanck.dae with BVH acceleration</figcaption>
      </td>
      <td>
        <img src="CBlucy.png" align="middle" width="400px"/>
        <figcaption align="middle">CBLucy.png with BVH acceleration</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2 align="middle">Part III: Direct Illumination</h2>

In direct lighting with uniform hemisphere sampling, we want to loop through the total number of samples to be taken, which is the product of the number of samples per area light source and the number of lights in the scene. Using a samplingHemisphere already created, we call a sample on that to get a random sample vector in the hemisphere. From that, we create a ray that has an origin of a hit point calculated in the given code and direction of the ray, calculated by transforming our random vector from object space to world space. We also set the ray’s beginning to EPS_F for numerical precision and prevent the ray from intersecting the surface it started from. Finally, we check for intersections using BVH acceleration and the ray we just created. If there is an intersection, we add to the running radiance total by multiplying the intersection’s emission, the sampled bsdf value of the passed in intersection, and the cosine of wi obtained from the previous bsdf sampling, and finally dividing by the pdf. This process uses a Monte Carlo estimator. After looping through the required number of samples, we normalize the total radiance and return. In direct lighting by importance sampling, we loop through each light within the scene and determine whether the light is a point source or not via SceneLight::is_delta_light(). If it is from a point source, then sample that light which returns a unit vector giving the sampled direction between the hit point and light source, the distance between the hit point in the unit vector’s direction, and probability density function in the unit vector’s direction. Just like in hemisphere sampling, we precalculated the hit point in the given code. With this information, we create a new ray with origin hit point and direction of the unit vector wi from the sample. We set its beginning to EPS_F for the same reason in the previous function and set its end to the difference of the distance and EPS_F to avoid intersecting the light with itself. If the light is behind the surface, then the cos of the wi unit vector is less than 0 and we don’t modify our output radiance. Else if it is not behind the surface and using the BVH acceleration the ray does not intersect, we use the same function used in the Monte Carlo estimator to update our radiance and move on to the next light. In the case where the light is not from a point source, we follow the same steps, but use the Monte Carlo estimator as a whole for number samples per area light source and add the normalized running total to the final radiance. Once we’ve looped over all the lights, we return the final radiance we’ve been adding to all along.

When we look above at our below renders, the render that uses lighting sampling produces a sharper, less grainy image compared to direct hemisphere sampling. First, we can examine the faces of the cube. In the direct hemisphere sampling result, we can see that the surfaces are very grainy and do not appear smooth in color. Second, the actual bunny and it’s shadow is quite grainy. These results can likely be explained by the fact that the direct lighting method specifically samples the light source, whereas the direct hemisphere sampler samples within the hemisphere, which has a lower probability of capturing the light distributions accurately.

  <div align="middle">
  <table style="width=100%">
	<tr align="center">
      <td>
        <img src="CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle">Direct Hemisphere Lighting with CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="bunny_64_32_final.png" align="middle" width="400px"/>
        <figcaption align="middle">Direct Lighting with CBbunny.dae</figcaption>
      </td>
    </tr>
    	<tr align="center">
      <td>
        <img src="CBspheres_H_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle">Direct Hemisphere Lighting with CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="spheres_64_32.png" align="middle" width="400px"/>
        <figcaption align="middle">Direct Lighting with CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
	<tr align="center">
<tr>
      <td>
        <img src="bunny_1_1.png" align="middle" width="400px"/>
        <figcaption align="middle">light rays=1 sample rate=1</figcaption>
      </td>
      <td>
        <img src="bunny_1_4.png" align="middle" width="400px"/>
        <figcaption align="middle">light rays=4 sample rate=1</figcaption>
      </td>
    </tr>
	  	<tr align="center">
    <br>
      <td>
        <img src="bunny_1_16.png" align="middle" width="400px"/>
        <figcaption align="middle">light rays=16 sample rate=1</figcaption>
      </td>
      <td>
        <img src="bunny_1_64.png" align="middle" width="400px"/>
        <figcaption align="middle">light rays=64 sample rate=1</figcaption>
      </td>
</tr>
	  </table>
</div>



<h3 align="middle">Part IV: Global Illumination</h3>

	Pixel sampling is a process used to get the final color for each pixel within an image. We take the work from the previous task and add on to that. To perform texture mapping, you create two new Vector2D objects that contain the passed in texture variables. We take how we calculated barycentric weights in the previous problem and multiply that by our u and v vectors. We take this vector and pass it into one of the sampling methods. 
  Sample nearest works by scaling the texture coordinates by height and width, then rounding the coordinates and getting the texel value. In other words, we get the color that’s closest to the pixel. Sample bilinear works by doing the same procedure but on the four closest texels and lerping the resulting colors. We get the texel of the bottom left, bottom right, top left, and top right and horizontally lerp the to get the top and bottom and finally vertically lerp those two to get the final color.
  Below we compare nearest sampling with bilinear sampling for a sample rate of 1 and 16 for each. The top row shows that with a sample rate of 1, bilinear sampling outperforms nearest sampling. On the left, the colors are more stark and sharp--it has not done such a great job at taking into consideration colors around it. Even when we increase to a sample rate of 16, both images do significantly improve and the quality difference between the two also significantly decreases, bilinear sampling outperforms nearest sampling by still having a smoother blend of pixel colors. So we can see that there is a large difference between the two methods when there’s a stark contrast in colors with a low sample rate.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image51.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Nearest sampling with sample rate=1</figcaption>
      </td>
      <td>
        <img src="image52.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear sampling with sample rate=1</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="image53.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Nearest sampling with sample rate=1</figcaption>
      </td>
      <td>
        <img src="image54.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear sampling with sample rate=16</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 6: "Level sampling" with mipmaps for texture mapping</h3>

Level sampling is the process in which we use mipmapping levels in texture mapping to determine which texel we get. This takes advantage of computing information that is closer with detail and not so much for stuff that’s less relevant, or further. In order to get level, we calculate the log base 2 of the square root of the norms of du and dv with respect to x and y. To obtain these values, we take the inverse system logic from earlier and calculate dx and dy values by adding one to the x and y parameters in different steps and taking the dot product. Finally, we take these new values and dot product them by the u and v vectors that were inputted and subtract them to get the final du/dx, dv/dx, du/dy, dv/dy values.
With regards to speed, pixel sampling is the fastest of the three. By only looking at one point and assigning a color, there is not much work involved. With regards to level sampling and number of samples per level, level sampling is faster because it does not need to look at multiple samples per pixel. With regards to memory usage, once, again, pixel sampling uses the least memory because you just need to store the point and color info. Level sampling has second most memory usage and number of samples per pixel is third because you begin to store more texture information and sample more points per pixel leading to storing significantly more info. With regards to antialiasing power, the number of samples per pixel provides the best results because of how it captures more samples per pixel, such as near edges, then it is level sampling, and finally pixel sampling is last because it is very basic. In conclusion, the number of samples per pixel takes more space and time, but gives us better results.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image61.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_NEAREST.</figcaption>
      </td>
      <td>
        <img src="image62.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_LINEAR</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="image63.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_NEAREST</figcaption>
      </td>
      <td>
        <img src="image64.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_LINEAR</figcaption>
      </td>
    </tr>
  </table>
</div>
</body>
</html>
