<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
</style>
<title>CS 184/284A Pathtracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>


<body>

<h1 align="middle">CS 184/284A: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Homework 3: Pathtracer</h1>
<h2 align="middle">Aishik Bhattacharyya</h2>

<br><br>

<div>

<h2 align="middle">Overview</h2>
	
In this project, I created my own rasterizer with various sampling methods and analyzed its efficacy in the context of every method. I was able to understand why some techniques are better from different perspectives like memory and speed. It was also really cool to zoom into the images and play around with different methods and see, in real time, the stark differences.

<h2 align="middle">Part I: Ray Generation and Scene Generation</h2>

In the rendering pipeline, ray generation and primitive intersection both play very important roles. For ray generation, the end goal is to transform a two-dimensional coordinate in image space to a three-dimensional coordinate in camera space. The z-coordinate in camera space is set to -1 to maintain a consistent depth as this direction represents the viewing direction. To transform the x-coordinate, we take the horizontal field of view value in radians, divide it by 2, and take the negative tangent. As seen in the provided Camera::configure() function, we also need to multiply this by 2. The rationale behind this is that our field of view values are represented as half angles. We follow the same steps for the vertical field of view to get a three-dimensional vector. To convert, we apply the camera-to-world rotation matrix and normalize the resulting value for consistency to finally get our ray direction. To sum up the resulting ray, we have found the normalized ray direction and also note the given camera position. We also set lower and upper bounds for the ray to maintain bounds and set intersection values later. Now, with ray generation complete, we must apply this method in a practical setting. We can use this to estimate the integral of radiance over a pixel using Monte Carlo. Using the provided function Pathtracer::get_sample(), we get a two-dimensional vector to offset from our origin. We normalize these coordinates by the width and height of the image and pass it into the previous function we created. Then, we take this result, get the estimated radiance and add it to a running total. Finally, when we’ve completed sampling, we divide the running total by the number of samples and it to our sample buffer.

Calculating primitive intersection parts of the rendering pipeline is also important because it determines what the camera captures, including important features such as shadows and visibility. In this section, we implement two methods regarding intersections with triangles and spheres. One of them simply checks whether or not an intersection exists, the other does that and also returns information regarding any such intersection. The triangle intersection algorithm implements the Moller Trumbore Algorithm. This algorithm relies on finding the barycentric coordinates to determine where the ray intersects the triangle. It also returns a value, t, which determines the magnitude in the ray where the intersection occurs.  We check to see if the value t is greater than or equal to 0 and within bounds of the min and max values of the ray. We also check if the barycentric coordinates are between 0 and 1, inclusive. If these are all true then, there is an intersection and we can calculate the normal there by multiplying the barycentric coordinates and the mesh normals of the triangle.

<div align="middle">
  <table style="width=100%">
    <tr>
	<td>
        <img src="CBEmpty.png" align="middle" width="400px"/>
        <figcaption align="middle">CBEmpty</figcaption>
      </td>
      <td>
        <img src="CBSpheres.png" align="middle" width="400px"/>
        <figcaption align="middle">CBSpheres</figcaption>
      </td>
    </tr>
  </table>
</div>


<h3 align="middle">Part 2: Antialiasing triangles</h3>

  As we saw in Part 1, when we implement simple rasterization, we can get jagged edges such that they are not smooth and very pixelated. Supersampling is a very useful tool because we examine multiple samples for each pixel, thus being able to provide more information about the image. This is particularly important on the edges of the triangle, where there isn’t just one color dominating the area. After implementing supersampling, we should not expect such a jagged edge, but rather some lighter colored pixels that smooth out the border.  
My supersampling algorithm first made sure to modify the resize the buffers by the sample rate. Previously, we were just filling in a color for each pixel so simply having the size of product width and height was enough. Now, we want to collect more data per point, which is the sample rate. As for modifying the Task 1 rasterization algorithm, I looped through two nested loops of size square root of the sample rate inside the two nested loops that loops over each bounding box of triangles. I modified the current point we are iterating over by adding the fraction of how much of the squared root sample rate we’ve iterated over. Using the same normal vectors we did in the previous task, we take the dot product of the line vector from this point to each input point and its normal vector. If all three dot products are greater than or equal to zero, that means this point is in the triangle. This is very similar to Task 1. However, a key difference here is that we no longer directly fill the pixel with a color, but add this color information to the sample buffer. When resolving to the frame buffer, I pulled the resulting colors from the buffer for each point, summed their RGB values, and averaged it to choose the final color for that pixel. For filling the pixel, I used the code from the spec to fill in pixel color if the point is on screen.

  <div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image21.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate 1 for test 4</figcaption>
      </td>
      <td>
        <img src="image22.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate 2 for test 4.</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="image23.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Sample rate 3 for test4.</figcaption>
      </td>
    </tr>
  </table>
</div>

  We are looking at the left corner of the red triangle. When the sample rate is equal to 1, only 1 sample is not enough to account for the abrupt decreasing color distribution from a sea of red. But, when we increase the sample rate to 4, we see the missing pixels now a light shade of red and better shows the picture. Yet, it isn’t “great.” In the lowest row, we see a row of pixels hanging on to the triangle by its right corner. This is not smooth and shows that even a sample of 4 isn’t enough. This part of the image is so sharp that only on the 16 samples version are all the images more connected, demonstrating that the more we sample, the better distribution of colors we get connecting all the pixels together because the algorithm has more information regarding its surroundings to fill in each pixel.

<h3 align="middle">Part 3: Transforms</h3>

  I modified my cubeman to be jumping in the air with his legs up and hands pointing upwards. To move the legs up in the air I rotate the bottom half of his legs. And to show the hands pointing up, I scaled the quadrilateral and rotated, as well as rotating the upper half of the hand. One of the legs overlaps with the other legs and I thought this was cool because you usually don’t see both legs in real life when you jump in the air like this.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image3.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Jumping cubeman</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2 align="middle">Section II: Sampling</h2>

<h3 align="middle">Part 4: Barycentric coordinates</h3>

  Barycentric coordinates are a way we use to plot points within a triangle. The goal of the equation is to find three constant coefficients representing the weights, or pull, each input point has on the current point. All weights must add to 1. This helps us color in the triangle more complexly because the shade of color being used is no longer a constant and varies throughout the triangle. For our implementation, we use the code from Task 2 and add modifications. In order to find the barycentric coordinates, we want to solve a system. So we take the inverse of a matrix containing information on the x, y, and z coordination of the 3 input points. Then we follow the code from Task 2 to check whether or not the point is within the triangle. If it is, we calculate the barycentric coordinates by multiplying the inverse matrix with the point we’re on. We then take each coordinate and multiply it by the color of each point that was passed in.
  Let’s look at the bottom left corner of the triangle below. That corner is blue. So any point in that area of the triangle has a larger weight of that point, also giving it a larger weight of that point’s color, thus making it more and more blue as you go closer to it. The same logic applies at the other points. Now, in the middle, when it is equidistant from all three points, there is no longer one defining color, but rather a light mesh of all three colors.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image41.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Example triangle</figcaption>
      </td>
      <td>
        <img src="image42.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Color wheel</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 5: "Pixel sampling" for texture mapping</h3>

	Pixel sampling is a process used to get the final color for each pixel within an image. We take the work from the previous task and add on to that. To perform texture mapping, you create two new Vector2D objects that contain the passed in texture variables. We take how we calculated barycentric weights in the previous problem and multiply that by our u and v vectors. We take this vector and pass it into one of the sampling methods. 
  Sample nearest works by scaling the texture coordinates by height and width, then rounding the coordinates and getting the texel value. In other words, we get the color that’s closest to the pixel. Sample bilinear works by doing the same procedure but on the four closest texels and lerping the resulting colors. We get the texel of the bottom left, bottom right, top left, and top right and horizontally lerp the to get the top and bottom and finally vertically lerp those two to get the final color.
  Below we compare nearest sampling with bilinear sampling for a sample rate of 1 and 16 for each. The top row shows that with a sample rate of 1, bilinear sampling outperforms nearest sampling. On the left, the colors are more stark and sharp--it has not done such a great job at taking into consideration colors around it. Even when we increase to a sample rate of 16, both images do significantly improve and the quality difference between the two also significantly decreases, bilinear sampling outperforms nearest sampling by still having a smoother blend of pixel colors. So we can see that there is a large difference between the two methods when there’s a stark contrast in colors with a low sample rate.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image51.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Nearest sampling with sample rate=1</figcaption>
      </td>
      <td>
        <img src="image52.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear sampling with sample rate=1</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="image53.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Nearest sampling with sample rate=1</figcaption>
      </td>
      <td>
        <img src="image54.PNG" align="middle" width="400px"/>
        <figcaption align="middle">Bilinear sampling with sample rate=16</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3 align="middle">Part 6: "Level sampling" with mipmaps for texture mapping</h3>

Level sampling is the process in which we use mipmapping levels in texture mapping to determine which texel we get. This takes advantage of computing information that is closer with detail and not so much for stuff that’s less relevant, or further. In order to get level, we calculate the log base 2 of the square root of the norms of du and dv with respect to x and y. To obtain these values, we take the inverse system logic from earlier and calculate dx and dy values by adding one to the x and y parameters in different steps and taking the dot product. Finally, we take these new values and dot product them by the u and v vectors that were inputted and subtract them to get the final du/dx, dv/dx, du/dy, dv/dy values.
With regards to speed, pixel sampling is the fastest of the three. By only looking at one point and assigning a color, there is not much work involved. With regards to level sampling and number of samples per level, level sampling is faster because it does not need to look at multiple samples per pixel. With regards to memory usage, once, again, pixel sampling uses the least memory because you just need to store the point and color info. Level sampling has second most memory usage and number of samples per pixel is third because you begin to store more texture information and sample more points per pixel leading to storing significantly more info. With regards to antialiasing power, the number of samples per pixel provides the best results because of how it captures more samples per pixel, such as near edges, then it is level sampling, and finally pixel sampling is last because it is very basic. In conclusion, the number of samples per pixel takes more space and time, but gives us better results.

<div align="middle">
  <table style="width=100%">
    <tr>
      <td>
        <img src="image61.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_NEAREST.</figcaption>
      </td>
      <td>
        <img src="image62.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_ZERO and P_LINEAR</figcaption>
      </td>
    </tr>
    <br>
    <tr>
      <td>
        <img src="image63.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_NEAREST</figcaption>
      </td>
      <td>
        <img src="image64.PNG" align="middle" width="400px"/>
        <figcaption align="middle">L_NEAREST and P_LINEAR</figcaption>
      </td>
    </tr>
  </table>
</div>
</body>
</html>
